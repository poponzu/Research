Epoch: 1, Loss: 0.9918802428245544
Epoch: 2, Loss: 0.9198345351219177
  2% 2/100 [00:01<01:08,  1.44it/s]
  3% 3/100 [00:02<01:21,  1.19it/s]
Traceback (most recent call last):
  File "/Users/daiki/PycharmProjects/pythonProject/Research/test.py", line 37, in <module>
    encoder, decoder, _, _ = quick_train.quick_train(model, train_set, verbose=True, epochs=100,
  File "/Users/daiki/PycharmProjects/pythonProject/Research/quick_train.py", line 94, in quick_train
    losses = train_model(model, train_set, verbose, lr, epochs, denoise)
  File "/Users/daiki/PycharmProjects/pythonProject/Research/quick_train.py", line 60, in train_model
    optimizer.step()
  File "/Users/daiki/PycharmProjects/pythonProject/Research/venv/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/Users/daiki/PycharmProjects/pythonProject/Research/venv/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/Users/daiki/PycharmProjects/pythonProject/Research/venv/lib/python3.9/site-packages/torch/optim/adam.py", line 141, in step
    F.adam(params_with_grad,
  File "/Users/daiki/PycharmProjects/pythonProject/Research/venv/lib/python3.9/site-packages/torch/optim/_functional.py", line 105, in adam
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
KeyboardInterrupt